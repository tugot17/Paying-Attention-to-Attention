{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human to machine date translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human</th>\n",
       "      <th>machine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>friday october 20 1995</td>\n",
       "      <td>1995-10-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19 may 2011</td>\n",
       "      <td>2011-05-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>monday december 19 1994</td>\n",
       "      <td>1994-12-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>saturday june 22 1991</td>\n",
       "      <td>1991-06-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>november 4 1999</td>\n",
       "      <td>1999-11-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>july 16 1980</td>\n",
       "      <td>1980-07-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>friday august 11 1995</td>\n",
       "      <td>1995-08-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>august 16 2015</td>\n",
       "      <td>2015-08-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>jul 2 1980</td>\n",
       "      <td>1980-07-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>13 aug 1997</td>\n",
       "      <td>1997-08-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        human     machine\n",
       "0      friday october 20 1995  1995-10-20\n",
       "1                 19 may 2011  2011-05-19\n",
       "2     monday december 19 1994  1994-12-19\n",
       "3       saturday june 22 1991  1991-06-22\n",
       "4             november 4 1999  1999-11-04\n",
       "...                       ...         ...\n",
       "9995             july 16 1980  1980-07-16\n",
       "9996    friday august 11 1995  1995-08-11\n",
       "9997           august 16 2015  2015-08-16\n",
       "9998               jul 2 1980  1980-07-02\n",
       "9999              13 aug 1997  1997-08-13\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = os.path.join('human-machine.csv')\n",
    "\n",
    "df = pd.read_csv(PATH, index_col=0)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for the training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computers don't understand letters, so we need numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDict:\n",
    "    def __init__(self):\n",
    "        self.char2index = {}\n",
    "        self.char2count = {}\n",
    "        self.index2char = {SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.n_chars = 2\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for c in sentence:\n",
    "            self.addChar(c)\n",
    "\n",
    "    def addChar(self, char):\n",
    "        if char not in self.char2index:\n",
    "            self.char2index[char] = self.n_chars\n",
    "            self.char2count[char] = 1\n",
    "            self.index2char[self.n_chars] = char\n",
    "            self.n_chars += 1\n",
    "        else:\n",
    "            self.char2count[char] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    df = pd.read_csv(PATH)\n",
    "    human = df[\"human\"].values\n",
    "    machine = df[\"machine\"].values\n",
    "    pairs = []\n",
    "    for i in range(len(human)):\n",
    "        pairs.append([human[i], machine[i]])\n",
    "\n",
    "    input_lang = CharDict()\n",
    "    output_lang = CharDict()\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input char indices: {'f': 2, 'r': 3, 'i': 4, 'd': 5, 'a': 6, 'y': 7, ' ': 8, 'o': 9, 'c': 10, 't': 11, 'b': 12, 'e': 13, '2': 14, '0': 15, '1': 16, '9': 17, '5': 18, 'm': 19, 'n': 20, '4': 21, 's': 22, 'u': 23, 'j': 24, 'v': 25, 'w': 26, '7': 27, 'l': 28, '8': 29, 'h': 30, '6': 31, 'p': 32, '3': 33, '.': 34, '/': 35, 'g': 36}\n",
      "\n",
      "Example pair: ['november 27 1999', '1999-11-27']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepare_data()\n",
    "\n",
    "print(f\"Input char indices: {input_lang.char2index}\")\n",
    "print()\n",
    "print(f\"Example pair: {pairs[2137]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexes_from_sentence(lang, sentence):\n",
    "    return [lang.char2index[char] for char in sentence]\n",
    "\n",
    "\n",
    "def variable_from_sentence(lang, sentence):\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    var = Variable(torch.LongTensor(indexes).view(-1, 1)).to(device)\n",
    "    # if USE_CUDA: var = var.cuda()\n",
    "    return var\n",
    "\n",
    "\n",
    "def variables_from_pair(pair):\n",
    "    input_variable = variable_from_sentence(input_lang, pair[0])\n",
    "    target_variable = variable_from_sentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human input november 27 1999\n",
      "Machine output 1999-11-27\n",
      "Real input length: 16, parsed input shhape: torch.Size([17, 1])\n",
      "Real output length: 10, parsed input shhape: torch.Size([11, 1])\n",
      "\n",
      "Parsed human input tensor([[20],\n",
      "        [ 9],\n",
      "        [25],\n",
      "        [13],\n",
      "        [19],\n",
      "        [12],\n",
      "        [13],\n",
      "        [ 3],\n",
      "        [ 8],\n",
      "        [14],\n",
      "        [27],\n",
      "        [ 8],\n",
      "        [16],\n",
      "        [17],\n",
      "        [17],\n",
      "        [17],\n",
      "        [ 1]])\n",
      "Parsed machine output tensor([[ 2],\n",
      "        [ 3],\n",
      "        [ 3],\n",
      "        [ 3],\n",
      "        [ 5],\n",
      "        [ 2],\n",
      "        [ 2],\n",
      "        [ 5],\n",
      "        [ 7],\n",
      "        [10],\n",
      "        [ 1]])\n"
     ]
    }
   ],
   "source": [
    "pair = ['november 27 1999', '1999-11-27']\n",
    "\n",
    "print(f\"Human input {pair[0]}\")\n",
    "print(f\"Machine output {pair[1]}\")\n",
    "\n",
    "input_variable, target_variable = variables_from_pair(pair)\n",
    "\n",
    "print(f\"Real input length: {len(pair[0])}, parsed input shhape: {input_variable.shape}\")\n",
    "print(f\"Real output length: {len(pair[1])}, parsed input shhape: {target_variable.shape}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(f\"Parsed human input {input_variable}\")\n",
    "print(f\"Parsed machine output {target_variable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "\n",
    "<img src=\"images/seq2seq.png\"/>\n",
    "\n",
    "1. Embed\n",
    "2. Encode\n",
    "3. Attend\n",
    "4. Predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "<img src=\"images/encoder-network.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chars in input data: 37\n",
      "Hidden state: tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
      "Output shape: torch.Size([1, 1, 64])\n",
      "Hidden state shape: torch.Size([1, 1, 64])\n",
      "Output: tensor([[[ 0.3003,  0.3720, -0.4427, -0.1028,  0.2320,  0.0239,  0.0718,\n",
      "           0.1705, -0.2583, -0.1321,  0.3137, -0.1586,  0.1116,  0.0844,\n",
      "           0.0929, -0.4645, -0.2766, -0.2381, -0.3277, -0.0791, -0.0965,\n",
      "           0.0575, -0.2665, -0.1581,  0.3741,  0.4428,  0.2818,  0.3196,\n",
      "           0.2240,  0.0154, -0.2693,  0.2234,  0.0667, -0.1813,  0.4438,\n",
      "          -0.1171,  0.1241,  0.1133, -0.4366, -0.0806,  0.0720,  0.0864,\n",
      "           0.1491,  0.2510, -0.2212, -0.2321, -0.0400,  0.3869, -0.1614,\n",
      "          -0.0415,  0.1677,  0.1361, -0.1520, -0.0230,  0.0703,  0.0366,\n",
      "           0.4113, -0.2659, -0.1486,  0.3658, -0.0920,  0.1823, -0.1348,\n",
      "          -0.3670]]], grad_fn=<StackBackward>)\n",
      "Hidden state: tensor([[[ 0.3003,  0.3720, -0.4427, -0.1028,  0.2320,  0.0239,  0.0718,\n",
      "           0.1705, -0.2583, -0.1321,  0.3137, -0.1586,  0.1116,  0.0844,\n",
      "           0.0929, -0.4645, -0.2766, -0.2381, -0.3277, -0.0791, -0.0965,\n",
      "           0.0575, -0.2665, -0.1581,  0.3741,  0.4428,  0.2818,  0.3196,\n",
      "           0.2240,  0.0154, -0.2693,  0.2234,  0.0667, -0.1813,  0.4438,\n",
      "          -0.1171,  0.1241,  0.1133, -0.4366, -0.0806,  0.0720,  0.0864,\n",
      "           0.1491,  0.2510, -0.2212, -0.2321, -0.0400,  0.3869, -0.1614,\n",
      "          -0.0415,  0.1677,  0.1361, -0.1520, -0.0230,  0.0703,  0.0366,\n",
      "           0.4113, -0.2659, -0.1486,  0.3658, -0.0920,  0.1823, -0.1348,\n",
      "          -0.3670]]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of chars in input data: {input_lang.n_chars}\")\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_chars, hidden_size=64).to(device)\n",
    "\n",
    "hidden = encoder.init_hidden()\n",
    "\n",
    "print(f\"Hidden state: {hidden}\")\n",
    "\n",
    "output, hidden = encoder(torch.tensor(21), hidden)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Hidden state shape: {hidden.shape}\")\n",
    "\n",
    "\n",
    "print(f\"Output: {output}\")\n",
    "print(f\"Hidden state: {hidden}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Example decoder in Encoder-Decoder architecture <b> WE DONT USE IT</b>, we will use Attention decoder instead. This one is only to introduce the concept of decoder.\n",
    "\n",
    "<img src=\"images/decoder-network.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chars in target data: 13\n",
      "Output shape: torch.Size([1, 13])\n",
      "Hidden state shape: torch.Size([1, 1, 64])\n",
      "Output: tensor([[-2.3642, -2.6364, -2.7685, -2.6342, -2.5088, -2.8177, -2.4102, -2.5503,\n",
      "         -2.5490, -2.4882, -2.4392, -2.6923, -2.5946]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Hidden state: tensor([[[ 0.1806,  0.2839,  0.0125,  0.0128,  0.3112,  0.0917, -0.0111,\n",
      "           0.2103,  0.0500, -0.3774,  0.3369, -0.2023,  0.1063,  0.1158,\n",
      "          -0.1134, -0.2829, -0.1823, -0.1188, -0.2500, -0.0831, -0.2683,\n",
      "           0.1887,  0.0972, -0.1092,  0.1787,  0.1853, -0.0783,  0.3028,\n",
      "           0.0659,  0.0028, -0.0637, -0.0154,  0.2989, -0.0052,  0.2274,\n",
      "          -0.0957,  0.2096,  0.1651, -0.4315, -0.1033,  0.0460, -0.0662,\n",
      "          -0.0218,  0.1258, -0.2830, -0.2432, -0.2243, -0.0790,  0.1172,\n",
      "          -0.0576, -0.1460,  0.0593,  0.0115, -0.2242,  0.1770,  0.0749,\n",
      "           0.0161,  0.0592,  0.2276,  0.4382, -0.3567,  0.0481, -0.3374,\n",
      "          -0.1825]]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of chars in target data: {output_lang.n_chars}\")\n",
    "\n",
    "\n",
    "decoder = DecoderRNN(64, output_lang.n_chars).to(device)\n",
    "\n",
    "# hidden = decoder.init_hidden()\n",
    "\n",
    "output, hidden = decoder(torch.tensor([[SOS_token]], device=device), hidden)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Hidden state shape: {hidden.shape}\")\n",
    "\n",
    "\n",
    "print(f\"Output: {output}\")\n",
    "print(f\"Hidden state: {hidden}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention decoder\n",
    "\n",
    "<img width=300px src=\"images/attention-decoder-network.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100_000\n",
    "LEARNING_RATE = 0.005\n",
    "HIDDEN_SIZE = 64\n",
    "TEACHER_FORCING_RATIO = 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "          criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[i], encoder_hidden)\n",
    "        encoder_outputs[i] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < TEACHER_FORCING_RATIO else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target  as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_lang.n_chars, HIDDEN_SIZE).to(device)\n",
    "decoder = AttentionDecoderRNN(HIDDEN_SIZE, output_lang.n_chars, dropout_p=0.1).to(device)\n",
    "\n",
    "\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=LEARNING_RATE)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pairs = [variables_from_pair(random.choice(pairs))\n",
    "                  for i in range(EPOCHS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr/anaconda3/envs/pumapython/lib/python3.7/site-packages/ipykernel_launcher.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254ec3b2ae174b2dbbcab0a4ea6215ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.5888137817382812\n",
      "Loss: 1.4269364963878284\n",
      "Loss: 0.850440805608576\n",
      "Loss: 0.6921418363397772\n",
      "Loss: 0.7046861215071245\n",
      "Loss: 0.5777382417158647\n",
      "Loss: 0.44399512897838245\n",
      "Loss: 0.2905229004946622\n",
      "Loss: 0.2166437885978005\n",
      "Loss: 0.38012929396195844\n",
      "Loss: 0.09716956181959673\n",
      "Loss: 0.14627281102267178\n",
      "Loss: 0.19291227514093573\n",
      "Loss: 0.08885291489687833\n",
      "Loss: 0.1483296047557484\n",
      "Loss: 0.025949554009871048\n",
      "Loss: 0.05035802992907437\n",
      "Loss: 0.006254009225151755\n",
      "Loss: 0.04519004713405262\n",
      "Loss: 0.009651270779696379\n",
      "Loss: 0.027279664169658314\n",
      "Loss: 0.07884387536482378\n",
      "Loss: 0.00984373688697815\n",
      "Loss: 0.008349692279642279\n",
      "Loss: 0.006630203263326125\n",
      "Loss: 0.01171667061068795\n",
      "Loss: 0.002110092138702219\n",
      "Loss: 0.0030849901112643156\n",
      "Loss: 0.006179376081986861\n",
      "Loss: 0.001530404795299877\n",
      "Loss: 0.10805143009532582\n",
      "Loss: 0.002618956464258107\n",
      "Loss: 0.00457262247800827\n",
      "Loss: 0.0010384129868312316\n",
      "Loss: 0.003936468877575614\n",
      "Loss: 0.003496804359284314\n",
      "Loss: 0.0007837569679726254\n",
      "Loss: 0.0014186434617096727\n",
      "Loss: 0.001364708116108721\n",
      "Loss: 0.0182010910727761\n",
      "Loss: 0.001026312227953564\n",
      "Loss: 0.0024515681646086955\n",
      "Loss: 0.003025562587109479\n",
      "Loss: 0.0014540929008613932\n",
      "Loss: 0.003691024739633907\n",
      "Loss: 0.0020988871428099546\n",
      "Loss: 0.0007405711168592626\n",
      "Loss: 0.013157589869065718\n",
      "Loss: 0.0008469373834404079\n",
      "Loss: 0.00031388929875736886\n",
      "Loss: 0.0009164195507764816\n",
      "Loss: 0.00018551522357897326\n",
      "Loss: 0.0014839041978120804\n",
      "Loss: 0.0006296042014252056\n",
      "Loss: 0.001055174930529161\n",
      "Loss: 0.0003820358436893333\n",
      "Loss: 0.0012539374557408419\n",
      "Loss: 0.00038864023306153035\n",
      "Loss: 0.0005903067067265511\n",
      "Loss: 0.00036152867092327637\n",
      "Loss: 0.00470814278179949\n",
      "Loss: 0.00514531982215968\n",
      "Loss: 0.0002919806336814707\n",
      "Loss: 0.0011670155112038958\n",
      "Loss: 0.0005148204297504642\n",
      "Loss: 0.0018121026117693293\n",
      "Loss: 0.0002636335451494564\n",
      "Loss: 0.00011534304146400906\n",
      "Loss: 0.0007138639180497689\n",
      "Loss: 0.0007777438414367763\n",
      "Loss: 0.0005131905729120427\n",
      "Loss: 0.001275260763412172\n",
      "Loss: 0.0016916519538922744\n",
      "Loss: 0.0016533395444804971\n",
      "Loss: 0.00021460140123963356\n",
      "Loss: 0.0023041902617974715\n",
      "Loss: 0.00018738148818639192\n",
      "Loss: 0.0004898645407096906\n",
      "Loss: 0.0007105800746516748\n",
      "Loss: 0.00011865852866321802\n",
      "Loss: 0.0007737806405533444\n",
      "Loss: 0.001480884850025177\n",
      "Loss: 0.0001513564811003479\n",
      "Loss: 0.0005515823564068837\n",
      "Loss: 0.0019373223185539246\n",
      "Loss: 0.00023466175083409655\n",
      "Loss: 0.00010371935257518834\n",
      "Loss: 8.137689755213532e-05\n",
      "Loss: 0.0007331400093707172\n",
      "Loss: 0.00035614693876017225\n",
      "Loss: 0.0007473579184575515\n",
      "Loss: 0.001604563979939981\n",
      "Loss: 0.0003580936569381844\n",
      "Loss: 0.0006893377497114918\n",
      "Loss: 0.0005975988354872574\n",
      "Loss: 0.001372093419459733\n",
      "Loss: 0.0003425074656578627\n",
      "Loss: 0.0001894197723066265\n",
      "Loss: 0.00018795598721639678\n",
      "Loss: 0.0003421347037973729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm_notebook(range(EPOCHS)):\n",
    "    \n",
    "    pair = training_pairs[epoch]\n",
    "    input_tensor = pair[0]\n",
    "    target_tensor = pair[1]\n",
    "    \n",
    "\n",
    "    \n",
    "    loss = train(input_tensor, target_tensor, encoder,\n",
    "             decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Loss: {loss}\")\n",
    "    \n",
    "    losses.append(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show predictions generated by a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    droput_value = decoder.dropout_p\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        decoder.dropout_p = 0.0\n",
    "        \n",
    "        input_tensor = variable_from_sentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for i in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[i],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[i] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2char[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            \n",
    "        decoder.dropout_p = droput_value\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(input_word, output_word, attentions):\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(attentions.numpy())\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(len(input_word)))\n",
    "    ax.set_yticks(np.arange(len(output_word)))\n",
    "    # ... and label them with the respective list entries\n",
    "    ax.set_xticklabels(input_word)\n",
    "    ax.set_yticklabels(output_word)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    \n",
    "    output_word = \"\".join(output_word).replace(\"<EOS>\", \"\")\n",
    "\n",
    "    ax.set_title(f\"Input: {input_word}, Prediction: {output_word}\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    \n",
    "    rcParams['figure.figsize'] = 10, 20\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_word = \"fr. 3 07 1997\"\n",
    "\n",
    "output_word, attentions = evaluate(\n",
    "    encoder, decoder, input_word)\n",
    "\n",
    "visualize_attention(input_word, output_word, attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
