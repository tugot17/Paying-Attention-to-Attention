# paying-attention-to-attention
Presentation and code about basics of attention mechanism in seq2seq models
