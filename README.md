# Paying attention to attention
Presentation and code about basics of attention mechanism in seq2seq models



## Credits

[Presentation](https://github.com/tugot17/paying-attention-to-attention/blob/master/Paying_attention_to_Attention.pdf) contains references to all images and papers used with it. 

The following data sources and articles were used to create the helper [notebook](https://github.com/tugot17/paying-attention-to-attention/blob/master/human_machine_translation.ipynb)

### Data 

Human-machine dataset was taken from [this repository](https://github.com/mzbac/human-to-machine-date-translation/blob/master/human-machine.csv)

### Pytorch official tutorial

The general idea was for seq2seq translation with attention mechanism was inspired by [official pytorch tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)


## Authors
* [tugot17](https://github.com/tugot17)

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details
